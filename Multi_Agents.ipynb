{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX0VptuiZlLp",
        "outputId": "09b66a66-f600-43f5-ccf3-9c2b208e6021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 2.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.3/67.3 kB 5.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 3.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.2/50.2 kB 3.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.8/131.8 kB 9.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 463.3/463.3 kB 17.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.9/210.9 kB 17.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 41.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 367.8/367.8 kB 24.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.0/64.0 kB 5.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.5/52.5 kB 4.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 8.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.7/149.7 kB 12.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 776.5/776.5 kB 45.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.8/144.8 kB 11.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.0/233.0 kB 18.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 525.5/525.5 kB 32.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 77.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 221.3/221.3 kB 17.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 96.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 kB 11.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.6/131.6 kB 11.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.4/76.4 kB 6.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 5.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 327.6/327.6 kB 24.7 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.1/115.1 kB 9.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.6/21.6 MB 98.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 80.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 396.4/396.4 kB 28.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.0/52.0 kB 4.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.4/290.4 kB 21.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.8/50.8 kB 3.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.2/54.2 kB 4.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.8/295.8 kB 24.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.1/71.1 kB 6.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 24.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 4.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 120.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 55.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 273.8/273.8 kB 21.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.2/139.2 kB 11.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.0/94.0 kB 7.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 90.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 73.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.2/207.2 kB 16.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.6/67.6 kB 5.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/13.2 MB 102.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.9/141.9 kB 11.7 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 103.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 259.4/259.4 kB 20.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 476.0/476.0 kB 32.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 5.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 6.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/12.5 MB 115.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 85.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 21.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 4.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.4/341.4 kB 22.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.3/49.3 kB 3.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 8.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.7/82.7 kB 6.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.4/71.4 kB 5.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 68.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 425.7/425.7 kB 27.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.3/157.3 kB 12.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 3.7 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 3.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 6.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.1/170.1 kB 3.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 51.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 85.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.4/194.4 kB 17.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 70.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip -qqq install 'crewai[tools]'\n",
        "pip -qqq install youtube-transcript-api\n",
        "pip -qqq install yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from crewai import Agent\n",
        "from google.colab import userdata\n",
        "from crewai import Crew,Process\n",
        "from crewai_tools import YoutubeChannelSearchTool\n",
        "from crewai import Task"
      ],
      "metadata": {
        "id": "rvn9lYp4aYh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "model_ID = userdata.get('GPT_MODEL')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "WSdVU8YMbKxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_MODEL_NAME']=model_ID"
      ],
      "metadata": {
        "id": "GGAORb3fb1Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai_tools import YoutubeVideoSearchTool"
      ],
      "metadata": {
        "id": "dIlwDaA2dSUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = YoutubeVideoSearchTool(youtube_video_url='https://www.youtube.com/watch?v=LfnrRPFhkuY')"
      ],
      "metadata": {
        "id": "k2EtDhanfJ6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFLZuPgAfR-x",
        "outputId": "f80e2447-415c-4ba1-f010-6f1de0d94233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YoutubeVideoSearchTool(name='Search a Youtube Video content', description=\"Search a Youtube Video content(search_query: 'string') - A tool that can be used to semantic search a query the https://www.youtube.com/watch?v=LfnrRPFhkuY Youtube Video content. search_query: 'Mandatory search query you want to use to search the Youtube Video content'\", args_schema=<class 'crewai_tools.tools.youtube_video_search_tool.youtube_video_search_tool.FixedYoutubeVideoSearchToolSchema'>, description_updated=False, cache_function=<function BaseTool.<lambda> at 0x7a6375711ab0>, result_as_answer=False, summarize=False, adapter=EmbedchainAdapter(embedchain_app=<embedchain.app.App object at 0x7a6372072f80>, summarize=False), config=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_reaearch = Agent(\n",
        "    role='Topic Research Agent',\n",
        "    goal='Search for relevant information on the topic {topic} from the provided youtube video',\n",
        "    verbose=True,\n",
        "    memory=True,\n",
        "    backstory='Expert in finding and analyzing relevant content from the youtube video, Specializing in AI, Data Science, Machine learning, and Generative AI',\n",
        "    tools=[tool],\n",
        "    allow_delegation=True\n",
        ")"
      ],
      "metadata": {
        "id": "e8HzE6MifVPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_post_agent = Agent(\n",
        "    role='Linkedin Post Creator',\n",
        "    goal='Write a concise LinkedIn Post summary from the transcription provided by the Topic Researcher',\n",
        "    verbose=True,\n",
        "    memory=True,\n",
        "    backstory='Expert in crafting engaging LinkedIn posts that summarize complex topics and include trending hashtags for maximum visibility',\n",
        "    #tools=[tool], no tools required\n",
        "    allow_delegation=False\n",
        ")"
      ],
      "metadata": {
        "id": "9eo7uy_eg_E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blog_writer = Agent(\n",
        "    role='Blog Writer',\n",
        "    goal='Write a comprehensive blog post from the transcription provided by the topic Researcher, covering all nessasary sections',\n",
        "    verbose=True,\n",
        "    memory=True,\n",
        "    backstory='Expert in creating in-depth, well stuctured blog posts that explain technical concepts clearly and engage readers from introduction to conclus ion',\n",
        "    #tools=[tool], no tools required\n",
        "    allow_delegation=False\n",
        ")"
      ],
      "metadata": {
        "id": "6T8H4ZAYiNnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "research_task = Task(\n",
        "    description = 'Identify and analyze video on the topic {topic} from the specified youtube video',\n",
        "    expected_output='A complete word by word report on the most important points in the video for the topic {topic}',\n",
        "    agent=topic_reaearch,\n",
        "    tools=[tool]\n",
        ")"
      ],
      "metadata": {
        "id": "1BDtdw38jEFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blog_writibg_task = Task(\n",
        "    description = '''Write a comprehensive blog post based on the transcription provided by the Topic Reasearcher\n",
        "                     article must include an introduction, step by step guides and conclusion. The overall content must be about 600 words long''',\n",
        "    expected_output='A markdown-formatted of the blog',\n",
        "    agent=blog_writer,\n",
        "    output_file = 'blog-post.md'\n",
        ")"
      ],
      "metadata": {
        "id": "R8k2dPfkkJyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_post_task = Task(\n",
        "    description = 'Write a linkedin post based on the transcription provided by the Topic Reasearcher, including relavant hashtags',\n",
        "    expected_output='A markdown-formatted of the LinkedIn post',\n",
        "    agent=linkedin_post_agent,\n",
        "    output_file = 'linkedin-post.md'\n",
        ")"
      ],
      "metadata": {
        "id": "vzKesrqhlvjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_crew = Crew(\n",
        "    agents=[topic_reaearch, blog_writer, linkedin_post_agent],\n",
        "    tasks=[research_task, blog_writibg_task, linkedin_post_task],\n",
        "    verbose=True,\n",
        "    memory = True,\n",
        "    cache = True,\n",
        "    share_crew=True\n",
        ")"
      ],
      "metadata": {
        "id": "PVgHxXtVmTAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic1 = 'LSTM'\n",
        "result = my_crew.kickoff(inputs={'topic':topic1})\n",
        "print('----------------------------------------')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz-cyRARmldC",
        "outputId": "1ec2409b-8c0b-4e49-f5ba-6bf4c787651b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m [2024-09-09 16:26:32][DEBUG]: == Working Agent: Topic Research Agent\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-09 16:26:32][INFO]: == Starting Task: Identify and analyze video on the topic LSTM from the specified youtube video\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to search the specified YouTube video for content related to the topic \"LSTM.\"\n",
            "\n",
            "Action: Search a Youtube Video content\n",
            "Action Input: {\"search_query\": \"LSTM\"}\u001b[0m\u001b[95m \n",
            "\n",
            "Relevant Content:\n",
            "Seen this movie called Memento or there is a bollywood movie called Ghajni? our basic RNNs are like the heroes of that movie they suffer from a short-term memory problem LSTM is a special version of RNN which solves the short-term memory problem and in this video I will explain LSTM in a very simple manner using real-life examples let's say you have NLP task for centers completion here in both the sentences based on what word appeared in the beginning your autocomplete sentence might be different for example for the first one I would say I need to take a loan whereas for the second one I would say I had to take a loan and this decision between need and head was made based on what appeared at the very beginning now looking at our traditional RNN and architecture if you have seen my previous videos you would know this is how RNN architecture looks like so if you're not seeing those videos I highly recommend you watch them because they are kind of a prerequisite here when you feed the sentence word by word okay so first you will feed today it will learn some weight there is an activation which is fed back now you uh work on the second word which is due then the third word which is two so this is how basic RNN works if you unroll this thing in time then this is how the architecture will look like. Many people get confused they think that this is a neural network with so many layers. Actually there is only one layer look at this time axis t1 t2 t3 so it's the same layer represented in different time and when you unroll it this is how it looks. now to predict this word need it needs to know about the word today which appeared at the very beginning of the sentence and because of vanishing gradient problem the traditional RNNs have short-term memory so they don't remember what appeared in the beginning of the sentence they have a very short-term memory of just few words which are like nearby, hence to autocomplete this kind of sentence RNN won't be able to do a good job.\n",
            "\n",
            "output uh you take that and then you take long term memory apply 10 h and you multiply that and you will get your hidden state okay so this will be your hidden state there are cases like let's say the sentence auto complete case that we're looking at at there is no yt actually the state is carried using the short-term memory but you know if there is a task like a named entity recognization there you need y t which is an output and that output is same as h t i mean you might apply sigmoid uh not sigmoid but let's say softmax type of function here uh but other than that it is kind of similar to h d so now if you think about the long-term memory so long-term memory look at this line okay look at this highway for example it has two things forget get an input brigade so forget get is like it will help you forget things like samosa when pasta comes in and the input gate will add new things into memory like meaningful things you want to add in a memory if it is a movie review you want to add like horrible or amazing beautiful those kind of words you know is the there are so many words you don't care about so that's what this lSTM will do and i'm going to refer you to a nice article i personally found this article to be very useful I'm going to link in the video description below I did not mention any mathematical equation because you can use this article for any math c here they explain how ct is calculated in terms of math formulas all math formulas again I suggest you read this article properly because I myself have learned a lot from this and I hope you found this video useful if you did please give it a thumbs up share it with your friends I think this this can provide you a simple explanation of LSTM and in the coming videos we'll be doing coding on an LSTM we will also be going over GRU in the future videos all right so till then thank you very much and bye\n",
            "\n",
            "cells are LSTM cells the first most important thing is the forget get. So the role of forget get is when you come at this word pasta it knows that it has to discard samosa okay so this is how forget get looks like so I have just expanded that cell xt is a one word you know you process sentence one word by one word and t is the timestamp so that's why x(t) so when you feed pasta into this forget get so the forget get is simple you have previous hidden state you take the current input which is your current word and you apply sigmoid function now you know that sigmoid function restricts your number between 0 and 1 so it will if it has to discard the previous memory it will output a vector which will have all zeros or all the values which are close to zero and when you multiply that with previous memory which is previous cell state you know you have a vector of let's say all zeros here and you have another vector which is a memory of previous cell state. the multiplication will of course be zero because you have discarded the previous memory. you know when this new word appeared so this is what a forget get looks like so here you forgot about samosa now there is another thing which is an input gate so when pasta came not only you forgot about samosa you need to add a memory of pasta so the way it will work is you will use sigmoid and tan h both on these two vectors okay so when you're doing by the way these vectors will have weights here so there will be some weight here some weight here so in this function what you are doing is h t minus 1 into that weight plus x t into that weight plus bias and then you are applying 10 h on top of it and it's the same equation here the only difference is instead of tan h you are using sigmoid function and you multiply both of this output and then add that as a memory for this word the third one is output gate so in the output gate again you are doing weighted sum of hidden state nxt and applying sigmoid function whatever is the\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mFinal Answer: \n",
            "\n",
            "Seen this movie called Memento or there is a bollywood movie called Ghajni? our basic RNNs are like the heroes of that movie they suffer from a short-term memory problem LSTM is a special version of RNN which solves the short-term memory problem and in this video I will explain LSTM in a very simple manner using real-life examples let's say you have NLP task for centers completion here in both the sentences based on what word appeared in the beginning your autocomplete sentence might be different for example for the first one I would say I need to take a loan whereas for the second one I would say I had to take a loan and this decision between need and head was made based on what appeared at the very beginning now looking at our traditional RNN and architecture if you have seen my previous videos you would know this is how RNN architecture looks like so if you're not seeing those videos I highly recommend you watch them because they are kind of a prerequisite here when you feed the sentence word by word okay so first you will feed today it will learn some weight there is an activation which is fed back now you uh work on the second word which is due then the third word which is two so this is how basic RNN works if you unroll this thing in time then this is how the architecture will look like. Many people get confused they think that this is a neural network with so many layers. Actually there is only one layer look at this time axis t1 t2 t3 so it's the same layer represented in different time and when you unroll it this is how it looks. now to predict this word need it needs to know about the word today which appeared at the very beginning of the sentence and because of vanishing gradient problem the traditional RNNs have short-term memory so they don't remember what appeared in the beginning of the sentence they have a very short-term memory of just few words which are like nearby, hence to autocomplete this kind of sentence RNN won't be able to do a good job.\n",
            "\n",
            "output uh you take that and then you take long term memory apply 10 h and you multiply that and you will get your hidden state okay so this will be your hidden state there are cases like let's say the sentence auto complete case that we're looking at at there is no yt actually the state is carried using the short-term memory but you know if there is a task like a named entity recognization there you need y t which is an output and that output is same as h t i mean you might apply sigmoid uh not sigmoid but let's say softmax type of function here uh but other than that it is kind of similar to h d so now if you think about the long-term memory so long-term memory look at this line okay look at this highway for example it has two things forget get an input brigade so forget get is like it will help you forget things like samosa when pasta comes in and the input gate will add new things into memory like meaningful things you want to add in a memory if it is a movie review you want to add like horrible or amazing beautiful those kind of words you know is the there are so many words you don't care about so that's what this lSTM will do and i'm going to refer you to a nice article i personally found this article to be very useful I'm going to link in the video description below I did not mention any mathematical equation because you can use this article for any math c here they explain how ct is calculated in terms of math formulas all math formulas again I suggest you read this article properly because I myself have learned a lot from this and I hope you found this video useful if you did please give it a thumbs up share it with your friends I think this this can provide you a simple explanation of LSTM and in the coming videos we'll be doing coding on an LSTM we will also be going over GRU in the future videos all right so till then thank you very much and bye\n",
            "\n",
            "cells are LSTM cells the first most important thing is the forget get. So the role of forget get is when you come at this word pasta it knows that it has to discard samosa okay so this is how forget get looks like so I have just expanded that cell xt is a one word you know you process sentence one word by one word and t is the timestamp so that's why x(t) so when you feed pasta into this forget get so the forget get is simple you have previous hidden state you take the current input which is your current word and you apply sigmoid function now you know that sigmoid function restricts your number between 0 and 1 so it will if it has to discard the previous memory it will output a vector which will have all zeros or all the values which are close to zero and when you multiply that with previous memory which is previous cell state you know you have a vector of let's say all zeros here and you have another vector which is a memory of previous cell state. the multiplication will of course be zero because you have discarded the previous memory. you know when this new word appeared so this is what a forget get looks like so here you forgot about samosa now there is another thing which is an input gate so when pasta came not only you forgot about samosa you need to add a memory of pasta so the way it will work is you will use sigmoid and tan h both on these two vectors okay so when you're doing by the way these vectors will have weights here so there will be some weight here some weight here so in this function what you are doing is h t minus 1 into that weight plus x t into that weight plus bias and then you are applying 10 h on top of it and it's the same equation here the only difference is instead of tan h you are using sigmoid function and you multiply both of this output and then add that as a memory for this word the third one is output gate so in the output gate again you are doing weighted sum of hidden state nxt and applying sigmoid function whatever is the\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-09 16:26:49][DEBUG]: == [Topic Research Agent] Task output: Seen this movie called Memento or there is a bollywood movie called Ghajni? our basic RNNs are like the heroes of that movie they suffer from a short-term memory problem LSTM is a special version of RNN which solves the short-term memory problem and in this video I will explain LSTM in a very simple manner using real-life examples let's say you have NLP task for centers completion here in both the sentences based on what word appeared in the beginning your autocomplete sentence might be different for example for the first one I would say I need to take a loan whereas for the second one I would say I had to take a loan and this decision between need and head was made based on what appeared at the very beginning now looking at our traditional RNN and architecture if you have seen my previous videos you would know this is how RNN architecture looks like so if you're not seeing those videos I highly recommend you watch them because they are kind of a prerequisite here when you feed the sentence word by word okay so first you will feed today it will learn some weight there is an activation which is fed back now you uh work on the second word which is due then the third word which is two so this is how basic RNN works if you unroll this thing in time then this is how the architecture will look like. Many people get confused they think that this is a neural network with so many layers. Actually there is only one layer look at this time axis t1 t2 t3 so it's the same layer represented in different time and when you unroll it this is how it looks. now to predict this word need it needs to know about the word today which appeared at the very beginning of the sentence and because of vanishing gradient problem the traditional RNNs have short-term memory so they don't remember what appeared in the beginning of the sentence they have a very short-term memory of just few words which are like nearby, hence to autocomplete this kind of sentence RNN won't be able to do a good job.\n",
            "\n",
            "output uh you take that and then you take long term memory apply 10 h and you multiply that and you will get your hidden state okay so this will be your hidden state there are cases like let's say the sentence auto complete case that we're looking at at there is no yt actually the state is carried using the short-term memory but you know if there is a task like a named entity recognization there you need y t which is an output and that output is same as h t i mean you might apply sigmoid uh not sigmoid but let's say softmax type of function here uh but other than that it is kind of similar to h d so now if you think about the long-term memory so long-term memory look at this line okay look at this highway for example it has two things forget get an input brigade so forget get is like it will help you forget things like samosa when pasta comes in and the input gate will add new things into memory like meaningful things you want to add in a memory if it is a movie review you want to add like horrible or amazing beautiful those kind of words you know is the there are so many words you don't care about so that's what this lSTM will do and i'm going to refer you to a nice article i personally found this article to be very useful I'm going to link in the video description below I did not mention any mathematical equation because you can use this article for any math c here they explain how ct is calculated in terms of math formulas all math formulas again I suggest you read this article properly because I myself have learned a lot from this and I hope you found this video useful if you did please give it a thumbs up share it with your friends I think this this can provide you a simple explanation of LSTM and in the coming videos we'll be doing coding on an LSTM we will also be going over GRU in the future videos all right so till then thank you very much and bye\n",
            "\n",
            "cells are LSTM cells the first most important thing is the forget get. So the role of forget get is when you come at this word pasta it knows that it has to discard samosa okay so this is how forget get looks like so I have just expanded that cell xt is a one word you know you process sentence one word by one word and t is the timestamp so that's why x(t) so when you feed pasta into this forget get so the forget get is simple you have previous hidden state you take the current input which is your current word and you apply sigmoid function now you know that sigmoid function restricts your number between 0 and 1 so it will if it has to discard the previous memory it will output a vector which will have all zeros or all the values which are close to zero and when you multiply that with previous memory which is previous cell state you know you have a vector of let's say all zeros here and you have another vector which is a memory of previous cell state. the multiplication will of course be zero because you have discarded the previous memory. you know when this new word appeared so this is what a forget get looks like so here you forgot about samosa now there is another thing which is an input gate so when pasta came not only you forgot about samosa you need to add a memory of pasta so the way it will work is you will use sigmoid and tan h both on these two vectors okay so when you're doing by the way these vectors will have weights here so there will be some weight here some weight here so in this function what you are doing is h t minus 1 into that weight plus x t into that weight plus bias and then you are applying 10 h on top of it and it's the same equation here the only difference is instead of tan h you are using sigmoid function and you multiply both of this output and then add that as a memory for this word the third one is output gate so in the output gate again you are doing weighted sum of hidden state nxt and applying sigmoid function whatever is the\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-09 16:26:49][DEBUG]: == Working Agent: Blog Writer\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-09 16:26:49][INFO]: == Starting Task: Write a comprehensive blog post based on the transcription provided by the Topic Reasearcher\n",
            "                     article must include an introduction, step by step guides and conclusion. The overall content must be about 600 words long\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n",
            "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  5.90it/s]\n",
            "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  6.41it/s]\n",
            "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI now can give a great answer\n",
            "\n",
            "Final Answer:\n",
            "```markdown\n",
            "# Understanding LSTM: A Solution to Short-Term Memory Issues in RNNs\n",
            "\n",
            "Have you ever watched the movie \"Memento\" or its Bollywood adaptation \"Ghajni\"? If you have, you'll recall how the protagonists struggle with short-term memory loss. Traditional Recurrent Neural Networks (RNNs) face a similar issue. They struggle to remember information from earlier in the sequence, which hampers their performance in tasks requiring long-term dependencies. Long Short-Term Memory (LSTM) networks, a special type of RNN, solve this problem. In this blog post, we'll break down LSTMs using real-life examples to make the concept easy to grasp.\n",
            "\n",
            "## Introduction to LSTM\n",
            "\n",
            "Imagine you have a natural language processing (NLP) task like sentence completion. The prediction of the next word in a sentence depends on the words that came before it. For instance, consider these two sentences:\n",
            "\n",
            "1. \"I need to take a loan.\"\n",
            "2. \"I had to take a loan.\"\n",
            "\n",
            "The choice between \"need\" and \"had\" depends on the context provided by the earlier words in the sentence. Traditional RNNs struggle with this kind of task because they suffer from short-term memory issues due to the vanishing gradient problem. This is where LSTMs come into play. LSTMs are designed to retain information for long periods, making them ideal for tasks that require long-term memory.\n",
            "\n",
            "## Traditional RNNs: A Brief Overview\n",
            "\n",
            "A traditional RNN processes a sequence of words one by one. For example, in the sentence \"Today is a sunny day,\" the RNN processes \"Today\" first, learns some weights, and then moves on to \"is,\" and so on. If you unroll an RNN in time, it appears as though it has many layers, but in reality, it has only one layer represented at different time steps (t1, t2, t3).\n",
            "\n",
            "The problem arises when the RNN tries to predict a word that depends on information from the beginning of the sentence. Due to the vanishing gradient problem, traditional RNNs forget earlier words and focus only on the most recent ones. This short-term memory issue makes them unsuitable for tasks that require understanding of the entire sentence.\n",
            "\n",
            "## How LSTMs Work\n",
            "\n",
            "LSTMs introduce a sophisticated mechanism to retain information over long periods. Let's break down the components of an LSTM cell:\n",
            "\n",
            "### 1. Forget Gate\n",
            "\n",
            "The forget gate decides what information to discard from the previous cell state. For example, if the sentence moves from discussing \"samosa\" to \"pasta,\" the forget gate discards the memory of \"samosa.\" It takes the previous hidden state and the current word as inputs, applies a sigmoid function, and outputs a vector of values between 0 and 1. Values close to 0 mean forgetting the information, while values close to 1 mean retaining it.\n",
            "\n",
            "### 2. Input Gate\n",
            "\n",
            "The input gate decides what new information to add to the cell state. When \"pasta\" appears, the input gate adds this new information to the memory. It uses a combination of sigmoid and tanh functions to decide which parts of the new information are important to keep.\n",
            "\n",
            "### 3. Output Gate\n",
            "\n",
            "The output gate determines what part of the cell state to output. It uses the current hidden state and the new word as inputs, applies a sigmoid function, and decides what information to pass to the next cell.\n",
            "\n",
            "### Step-by-Step LSTM Process\n",
            "\n",
            "1. **Input Processing**: The current word and the previous hidden state are processed through the forget gate to decide what to discard.\n",
            "2. **Memory Update**: The input gate updates the cell state with new information.\n",
            "3. **Output Generation**: The output gate generates the output based on the updated cell state.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "LSTM networks are powerful tools for tasks that require long-term memory, such as sentence completion, named entity recognition, and more. By using gates to control the flow of information, LSTMs can retain important information over long periods, making them far more effective than traditional RNNs for certain tasks.\n",
            "\n",
            "If you found this explanation helpful, please give it a thumbs up and share it with your friends. Stay tuned for upcoming videos where we'll dive into coding with LSTMs and explore other advanced topics like Gated Recurrent Units (GRUs).\n",
            "\n",
            "Thank you for reading!\n",
            "\n",
            "---\n",
            "\n",
            "*Note: For those interested in the mathematical details of LSTMs, I recommend checking out the linked article in the video description. It provides an in-depth explanation of the formulas and calculations involved.*\n",
            "\n",
            "```\n",
            "\n",
            "This blog post provides a comprehensive explanation of LSTMs, including an introduction, a comparison with traditional RNNs, a detailed step-by-step guide on how LSTMs work, and a conclusion. The content is designed to be engaging and easy to understand, making complex technical concepts accessible to readers.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-09 16:27:02][DEBUG]: == [Blog Writer] Task output: ```markdown\n",
            "# Understanding LSTM: A Solution to Short-Term Memory Issues in RNNs\n",
            "\n",
            "Have you ever watched the movie \"Memento\" or its Bollywood adaptation \"Ghajni\"? If you have, you'll recall how the protagonists struggle with short-term memory loss. Traditional Recurrent Neural Networks (RNNs) face a similar issue. They struggle to remember information from earlier in the sequence, which hampers their performance in tasks requiring long-term dependencies. Long Short-Term Memory (LSTM) networks, a special type of RNN, solve this problem. In this blog post, we'll break down LSTMs using real-life examples to make the concept easy to grasp.\n",
            "\n",
            "## Introduction to LSTM\n",
            "\n",
            "Imagine you have a natural language processing (NLP) task like sentence completion. The prediction of the next word in a sentence depends on the words that came before it. For instance, consider these two sentences:\n",
            "\n",
            "1. \"I need to take a loan.\"\n",
            "2. \"I had to take a loan.\"\n",
            "\n",
            "The choice between \"need\" and \"had\" depends on the context provided by the earlier words in the sentence. Traditional RNNs struggle with this kind of task because they suffer from short-term memory issues due to the vanishing gradient problem. This is where LSTMs come into play. LSTMs are designed to retain information for long periods, making them ideal for tasks that require long-term memory.\n",
            "\n",
            "## Traditional RNNs: A Brief Overview\n",
            "\n",
            "A traditional RNN processes a sequence of words one by one. For example, in the sentence \"Today is a sunny day,\" the RNN processes \"Today\" first, learns some weights, and then moves on to \"is,\" and so on. If you unroll an RNN in time, it appears as though it has many layers, but in reality, it has only one layer represented at different time steps (t1, t2, t3).\n",
            "\n",
            "The problem arises when the RNN tries to predict a word that depends on information from the beginning of the sentence. Due to the vanishing gradient problem, traditional RNNs forget earlier words and focus only on the most recent ones. This short-term memory issue makes them unsuitable for tasks that require understanding of the entire sentence.\n",
            "\n",
            "## How LSTMs Work\n",
            "\n",
            "LSTMs introduce a sophisticated mechanism to retain information over long periods. Let's break down the components of an LSTM cell:\n",
            "\n",
            "### 1. Forget Gate\n",
            "\n",
            "The forget gate decides what information to discard from the previous cell state. For example, if the sentence moves from discussing \"samosa\" to \"pasta,\" the forget gate discards the memory of \"samosa.\" It takes the previous hidden state and the current word as inputs, applies a sigmoid function, and outputs a vector of values between 0 and 1. Values close to 0 mean forgetting the information, while values close to 1 mean retaining it.\n",
            "\n",
            "### 2. Input Gate\n",
            "\n",
            "The input gate decides what new information to add to the cell state. When \"pasta\" appears, the input gate adds this new information to the memory. It uses a combination of sigmoid and tanh functions to decide which parts of the new information are important to keep.\n",
            "\n",
            "### 3. Output Gate\n",
            "\n",
            "The output gate determines what part of the cell state to output. It uses the current hidden state and the new word as inputs, applies a sigmoid function, and decides what information to pass to the next cell.\n",
            "\n",
            "### Step-by-Step LSTM Process\n",
            "\n",
            "1. **Input Processing**: The current word and the previous hidden state are processed through the forget gate to decide what to discard.\n",
            "2. **Memory Update**: The input gate updates the cell state with new information.\n",
            "3. **Output Generation**: The output gate generates the output based on the updated cell state.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "LSTM networks are powerful tools for tasks that require long-term memory, such as sentence completion, named entity recognition, and more. By using gates to control the flow of information, LSTMs can retain important information over long periods, making them far more effective than traditional RNNs for certain tasks.\n",
            "\n",
            "If you found this explanation helpful, please give it a thumbs up and share it with your friends. Stay tuned for upcoming videos where we'll dive into coding with LSTMs and explore other advanced topics like Gated Recurrent Units (GRUs).\n",
            "\n",
            "Thank you for reading!\n",
            "\n",
            "---\n",
            "\n",
            "*Note: For those interested in the mathematical details of LSTMs, I recommend checking out the linked article in the video description. It provides an in-depth explanation of the formulas and calculations involved.*\n",
            "\n",
            "```\n",
            "\n",
            "This blog post provides a comprehensive explanation of LSTMs, including an introduction, a comparison with traditional RNNs, a detailed step-by-step guide on how LSTMs work, and a conclusion. The content is designed to be engaging and easy to understand, making complex technical concepts accessible to readers.\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-09 16:27:02][DEBUG]: == Working Agent: Linkedin Post Creator\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-09 16:27:02][INFO]: == Starting Task: Write a linkedin post based on the transcription provided by the Topic Reasearcher, including relavant hashtags\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI now can give a great answer.\n",
            "\n",
            "Final Answer: my best complete final answer to the task.\n",
            "\n",
            "```markdown\n",
            "🚀 **Understanding LSTM: A Solution to Short-Term Memory Issues in RNNs** 🚀\n",
            "\n",
            "Have you ever watched the movie \"Memento\" or its Bollywood adaptation \"Ghajni\"? The protagonists struggle with short-term memory loss, similar to how traditional Recurrent Neural Networks (RNNs) struggle to remember information from earlier in the sequence. This hampers their performance in tasks requiring long-term dependencies. Enter Long Short-Term Memory (LSTM) networks, a special type of RNN designed to retain information for long periods. In this post, we'll break down LSTMs using real-life examples to make the concept easy to grasp.\n",
            "\n",
            "### 📚 Introduction to LSTM 📚\n",
            "\n",
            "Imagine you have a natural language processing (NLP) task like sentence completion. The prediction of the next word in a sentence depends on the words that came before it. For instance:\n",
            "1. \"I need to take a loan.\"\n",
            "2. \"I had to take a loan.\"\n",
            "   \n",
            "Traditional RNNs struggle with this kind of task because they suffer from short-term memory issues due to the vanishing gradient problem. This is where LSTMs come into play. LSTMs are designed to retain information for long periods, making them ideal for tasks that require long-term memory.\n",
            "\n",
            "### 🧠 Traditional RNNs: A Brief Overview 🧠\n",
            "\n",
            "A traditional RNN processes a sequence of words one by one. For example, in the sentence \"Today is a sunny day,\" the RNN processes \"Today\" first, learns some weights, and then moves on to \"is,\" and so on. The problem arises when the RNN tries to predict a word that depends on information from the beginning of the sentence. Due to the vanishing gradient problem, traditional RNNs forget earlier words and focus only on the most recent ones.\n",
            "\n",
            "### 🔍 How LSTMs Work 🔍\n",
            "\n",
            "LSTMs introduce a sophisticated mechanism to retain information over long periods. Let's break down the components of an LSTM cell:\n",
            "\n",
            "1. **Forget Gate**: Decides what information to discard from the previous cell state.\n",
            "2. **Input Gate**: Decides what new information to add to the cell state.\n",
            "3. **Output Gate**: Determines what part of the cell state to output.\n",
            "\n",
            "### 🔄 Step-by-Step LSTM Process 🔄\n",
            "\n",
            "1. **Input Processing**: The current word and the previous hidden state are processed through the forget gate to decide what to discard.\n",
            "2. **Memory Update**: The input gate updates the cell state with new information.\n",
            "3. **Output Generation**: The output gate generates the output based on the updated cell state.\n",
            "\n",
            "### 🎯 Conclusion 🎯\n",
            "\n",
            "LSTM networks are powerful tools for tasks that require long-term memory, such as sentence completion, named entity recognition, and more. By using gates to control the flow of information, LSTMs can retain important information over long periods, making them far more effective than traditional RNNs for certain tasks.\n",
            "\n",
            "If you found this explanation helpful, please give it a thumbs up and share it with your network. Stay tuned for upcoming posts where we'll dive into coding with LSTMs and explore other advanced topics like Gated Recurrent Units (GRUs).\n",
            "\n",
            "Thank you for reading!\n",
            "\n",
            "#AI #MachineLearning #DeepLearning #RNN #LSTM #NLP #DataScience #Tech\n",
            "\n",
            "---\n",
            "\n",
            "*Note: For those interested in the mathematical details of LSTMs, I recommend checking out the linked article in the video description. It provides an in-depth explanation of the formulas and calculations involved.*\n",
            "```\n",
            "\n",
            "This LinkedIn post is designed to be engaging and accessible, making complex technical concepts easy to understand, and includes trending hashtags for maximum visibility.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-09 16:27:24][DEBUG]: == [Linkedin Post Creator] Task output: my best complete final answer to the task.\n",
            "\n",
            "```markdown\n",
            "🚀 **Understanding LSTM: A Solution to Short-Term Memory Issues in RNNs** 🚀\n",
            "\n",
            "Have you ever watched the movie \"Memento\" or its Bollywood adaptation \"Ghajni\"? The protagonists struggle with short-term memory loss, similar to how traditional Recurrent Neural Networks (RNNs) struggle to remember information from earlier in the sequence. This hampers their performance in tasks requiring long-term dependencies. Enter Long Short-Term Memory (LSTM) networks, a special type of RNN designed to retain information for long periods. In this post, we'll break down LSTMs using real-life examples to make the concept easy to grasp.\n",
            "\n",
            "### 📚 Introduction to LSTM 📚\n",
            "\n",
            "Imagine you have a natural language processing (NLP) task like sentence completion. The prediction of the next word in a sentence depends on the words that came before it. For instance:\n",
            "1. \"I need to take a loan.\"\n",
            "2. \"I had to take a loan.\"\n",
            "   \n",
            "Traditional RNNs struggle with this kind of task because they suffer from short-term memory issues due to the vanishing gradient problem. This is where LSTMs come into play. LSTMs are designed to retain information for long periods, making them ideal for tasks that require long-term memory.\n",
            "\n",
            "### 🧠 Traditional RNNs: A Brief Overview 🧠\n",
            "\n",
            "A traditional RNN processes a sequence of words one by one. For example, in the sentence \"Today is a sunny day,\" the RNN processes \"Today\" first, learns some weights, and then moves on to \"is,\" and so on. The problem arises when the RNN tries to predict a word that depends on information from the beginning of the sentence. Due to the vanishing gradient problem, traditional RNNs forget earlier words and focus only on the most recent ones.\n",
            "\n",
            "### 🔍 How LSTMs Work 🔍\n",
            "\n",
            "LSTMs introduce a sophisticated mechanism to retain information over long periods. Let's break down the components of an LSTM cell:\n",
            "\n",
            "1. **Forget Gate**: Decides what information to discard from the previous cell state.\n",
            "2. **Input Gate**: Decides what new information to add to the cell state.\n",
            "3. **Output Gate**: Determines what part of the cell state to output.\n",
            "\n",
            "### 🔄 Step-by-Step LSTM Process 🔄\n",
            "\n",
            "1. **Input Processing**: The current word and the previous hidden state are processed through the forget gate to decide what to discard.\n",
            "2. **Memory Update**: The input gate updates the cell state with new information.\n",
            "3. **Output Generation**: The output gate generates the output based on the updated cell state.\n",
            "\n",
            "### 🎯 Conclusion 🎯\n",
            "\n",
            "LSTM networks are powerful tools for tasks that require long-term memory, such as sentence completion, named entity recognition, and more. By using gates to control the flow of information, LSTMs can retain important information over long periods, making them far more effective than traditional RNNs for certain tasks.\n",
            "\n",
            "If you found this explanation helpful, please give it a thumbs up and share it with your network. Stay tuned for upcoming posts where we'll dive into coding with LSTMs and explore other advanced topics like Gated Recurrent Units (GRUs).\n",
            "\n",
            "Thank you for reading!\n",
            "\n",
            "#AI #MachineLearning #DeepLearning #RNN #LSTM #NLP #DataScience #Tech\n",
            "\n",
            "---\n",
            "\n",
            "*Note: For those interested in the mathematical details of LSTMs, I recommend checking out the linked article in the video description. It provides an in-depth explanation of the formulas and calculations involved.*\n",
            "```\n",
            "\n",
            "This LinkedIn post is designed to be engaging and accessible, making complex technical concepts easy to understand, and includes trending hashtags for maximum visibility.\n",
            "\n",
            "\u001b[00m\n",
            "----------------------------------------\n",
            "my best complete final answer to the task.\n",
            "\n",
            "```markdown\n",
            "🚀 **Understanding LSTM: A Solution to Short-Term Memory Issues in RNNs** 🚀\n",
            "\n",
            "Have you ever watched the movie \"Memento\" or its Bollywood adaptation \"Ghajni\"? The protagonists struggle with short-term memory loss, similar to how traditional Recurrent Neural Networks (RNNs) struggle to remember information from earlier in the sequence. This hampers their performance in tasks requiring long-term dependencies. Enter Long Short-Term Memory (LSTM) networks, a special type of RNN designed to retain information for long periods. In this post, we'll break down LSTMs using real-life examples to make the concept easy to grasp.\n",
            "\n",
            "### 📚 Introduction to LSTM 📚\n",
            "\n",
            "Imagine you have a natural language processing (NLP) task like sentence completion. The prediction of the next word in a sentence depends on the words that came before it. For instance:\n",
            "1. \"I need to take a loan.\"\n",
            "2. \"I had to take a loan.\"\n",
            "   \n",
            "Traditional RNNs struggle with this kind of task because they suffer from short-term memory issues due to the vanishing gradient problem. This is where LSTMs come into play. LSTMs are designed to retain information for long periods, making them ideal for tasks that require long-term memory.\n",
            "\n",
            "### 🧠 Traditional RNNs: A Brief Overview 🧠\n",
            "\n",
            "A traditional RNN processes a sequence of words one by one. For example, in the sentence \"Today is a sunny day,\" the RNN processes \"Today\" first, learns some weights, and then moves on to \"is,\" and so on. The problem arises when the RNN tries to predict a word that depends on information from the beginning of the sentence. Due to the vanishing gradient problem, traditional RNNs forget earlier words and focus only on the most recent ones.\n",
            "\n",
            "### 🔍 How LSTMs Work 🔍\n",
            "\n",
            "LSTMs introduce a sophisticated mechanism to retain information over long periods. Let's break down the components of an LSTM cell:\n",
            "\n",
            "1. **Forget Gate**: Decides what information to discard from the previous cell state.\n",
            "2. **Input Gate**: Decides what new information to add to the cell state.\n",
            "3. **Output Gate**: Determines what part of the cell state to output.\n",
            "\n",
            "### 🔄 Step-by-Step LSTM Process 🔄\n",
            "\n",
            "1. **Input Processing**: The current word and the previous hidden state are processed through the forget gate to decide what to discard.\n",
            "2. **Memory Update**: The input gate updates the cell state with new information.\n",
            "3. **Output Generation**: The output gate generates the output based on the updated cell state.\n",
            "\n",
            "### 🎯 Conclusion 🎯\n",
            "\n",
            "LSTM networks are powerful tools for tasks that require long-term memory, such as sentence completion, named entity recognition, and more. By using gates to control the flow of information, LSTMs can retain important information over long periods, making them far more effective than traditional RNNs for certain tasks.\n",
            "\n",
            "If you found this explanation helpful, please give it a thumbs up and share it with your network. Stay tuned for upcoming posts where we'll dive into coding with LSTMs and explore other advanced topics like Gated Recurrent Units (GRUs).\n",
            "\n",
            "Thank you for reading!\n",
            "\n",
            "#AI #MachineLearning #DeepLearning #RNN #LSTM #NLP #DataScience #Tech\n",
            "\n",
            "---\n",
            "\n",
            "*Note: For those interested in the mathematical details of LSTMs, I recommend checking out the linked article in the video description. It provides an in-depth explanation of the formulas and calculations involved.*\n",
            "```\n",
            "\n",
            "This LinkedIn post is designed to be engaging and accessible, making complex technical concepts easy to understand, and includes trending hashtags for maximum visibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1PwsHPpnLwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0adSyHmxm7Yi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}